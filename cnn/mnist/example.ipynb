{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets        import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import label_binarize\n",
    "\n",
    "PIXEL_RANGE = range(0, 255 + 1)\n",
    "IMAGE_RANGE = range(0, 9 + 1)\n",
    "\n",
    "def process(mnist):\n",
    "    # Process dataset to return features and labels for CNN\n",
    "    def features():\n",
    "        # Transform features to be float32 sets of 1x28x28 \n",
    "        # tensors with normalized pixel values.\n",
    "        return np.divide(\n",
    "            mnist.data, PIXEL_RANGE[-1]\n",
    "        ).astype(np.float32).reshape((-1, 1, 28, 28))\n",
    "    def labels():\n",
    "        # Transform labels to be float32 sets of 1x10\n",
    "        # tensors that are one-hot encoded.\n",
    "        return label_binarize(\n",
    "            mnist.target, classes=IMAGE_RANGE\n",
    "        ).astype(np.float32).reshape((-1, 10))\n",
    "    return features(), labels()\n",
    "\n",
    "\n",
    "# Download the MNIST dataset\n",
    "mnist            = fetch_mldata('MNIST original', data_home='.')\n",
    "features, labels = process(mnist)\n",
    "\n",
    "# Split train and test data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNTK 2.0 CPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import CNTK\n",
    "import cntk as c\n",
    "\n",
    "# Display CNTK version\n",
    "' '.join([\n",
    "    c.__name__.upper(),\n",
    "    c.__version__,\n",
    "    str(c.device.all_devices()[0])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet():\n",
    "    IMAGE_SHAPE        = (1, 28, 28)\n",
    "    IMAGE_CLASSES      = [n for n in IMAGE_RANGE]\n",
    "    IMAGE_CLASS_COUNT  = len(IMAGE_CLASSES)\n",
    "    LEARNING_RATE      = [0.2] * 20 + [0.1] * 20 + [0.0001] * 20 + [0.00001] * 20\n",
    "    EPOCH_COUNT        = 80\n",
    "    BATCH_SIZE         = 512\n",
    "    DROP_RATE          = 0.8\n",
    "    HIDDEN_LAYER_COUNT = 96\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._build_inputs()\n",
    "        self._build_layers()\n",
    "        self._build_stack()\n",
    "        self._build_trainer()\n",
    "    \n",
    "    def evaluate(self, fn, feeds):\n",
    "        for epoch in range(self.EPOCH_COUNT):\n",
    "            batch = self.batch(epoch, feeds)\n",
    "            fn(epoch, batch)\n",
    "    \n",
    "    def train(self, data):\n",
    "        self.trainer.train_minibatch(data)\n",
    "    \n",
    "    def summarize(self):\n",
    "        self.trainer.summarize_training_progress()\n",
    "    \n",
    "    def log_parameters(self):\n",
    "        c.logging.log_number_of_parameters(self.stack)\n",
    "        print()\n",
    "    \n",
    "    def checkpoint(self, version):\n",
    "        file_names = ['ConvNet', 'MNIST', '{}.dnn'.format(version)]\n",
    "        self.stack.save(\n",
    "            os.path.join('.', 'checkpoints', '_'.join(file_names))\n",
    "        )\n",
    "    \n",
    "    def batch(self, epoch, data):\n",
    "        def chunk(data):\n",
    "            slice_begin = epoch * self.BATCH_SIZE\n",
    "            slice_end   = slice_begin + self.BATCH_SIZE\n",
    "            return data[slice_begin:slice_end]\n",
    "        return {key: chunk(value) for key, value in data.items()}\n",
    "    \n",
    "    def _build_inputs(self):\n",
    "        self.inputs = c.input_variable(self.IMAGE_SHAPE,       np.float32, name='inputs')\n",
    "        self.labels = c.input_variable(self.IMAGE_CLASS_COUNT, np.float32, name='labels')\n",
    "    \n",
    "    def _build_layers(self):\n",
    "        with c.layers.default_options(activation=c.ops.relu, pad=False):\n",
    "            self.layers  = [\n",
    "                c.layers.Convolution2D((5,5), 32, pad=True),\n",
    "                c.layers.MaxPooling((3,3), (2,2)),\n",
    "                c.layers.Convolution2D((3,3), 48),\n",
    "                c.layers.MaxPooling((3,3), (2,2)),\n",
    "                c.layers.Convolution2D((3,3), 64),\n",
    "                c.layers.Dense(self.HIDDEN_LAYER_COUNT),\n",
    "                c.layers.Dropout(self.DROP_RATE),\n",
    "                c.layers.Dense(self.IMAGE_CLASS_COUNT, activation=None)\n",
    "            ]\n",
    "    \n",
    "    def _build_stack(self):\n",
    "        self.stack = self.inputs\n",
    "        for layer in self.layers: self.stack = layer(self.stack)\n",
    "        self.loss  = c.losses.cross_entropy_with_softmax(self.stack, self.labels)\n",
    "        self.error = c.metrics.classification_error(self.stack, self.labels)\n",
    "    \n",
    "    def _build_trainer(self):\n",
    "        schedule     = c.learning_rate_schedule(self.LEARNING_RATE, c.UnitType.minibatch)\n",
    "        learner      = c.learners.sgd(self.stack.parameters, schedule)\n",
    "        printer      = c.logging.ProgressPrinter(tag='Training', num_epochs=self.EPOCH_COUNT)\n",
    "        self.trainer = c.Trainer(self.stack, (self.loss, self.error), learner, printer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 98778 parameters in 10 parameter tensors.\n",
      "\n",
      "Learning rate per minibatch: 0.2\n",
      "Finished Epoch[1 of 80]: [Training] loss = 2.320826 * 512, metric = 91.41% * 512 1.426s (359.0 samples/s);\n",
      "Finished Epoch[2 of 80]: [Training] loss = 2.301237 * 512, metric = 90.82% * 512 0.962s (532.2 samples/s);\n",
      "Finished Epoch[3 of 80]: [Training] loss = 2.297102 * 512, metric = 88.28% * 512 0.943s (542.9 samples/s);\n",
      "Finished Epoch[4 of 80]: [Training] loss = 2.283885 * 512, metric = 87.11% * 512 0.957s (535.0 samples/s);\n",
      "Finished Epoch[5 of 80]: [Training] loss = 2.292555 * 512, metric = 89.65% * 512 0.990s (517.2 samples/s);\n",
      "Finished Epoch[6 of 80]: [Training] loss = 2.278016 * 512, metric = 87.11% * 512 0.925s (553.5 samples/s);\n",
      "Finished Epoch[7 of 80]: [Training] loss = 2.270154 * 512, metric = 86.72% * 512 1.238s (413.6 samples/s);\n",
      "Finished Epoch[8 of 80]: [Training] loss = 2.266858 * 512, metric = 87.11% * 512 1.089s (470.2 samples/s);\n",
      "Finished Epoch[9 of 80]: [Training] loss = 2.259655 * 512, metric = 84.18% * 512 0.976s (524.6 samples/s);\n",
      "Finished Epoch[10 of 80]: [Training] loss = 2.275764 * 512, metric = 85.16% * 512 0.938s (545.8 samples/s);\n",
      "Finished Epoch[11 of 80]: [Training] loss = 2.243586 * 512, metric = 83.01% * 512 0.940s (544.7 samples/s);\n",
      "Finished Epoch[12 of 80]: [Training] loss = 2.225177 * 512, metric = 82.42% * 512 1.059s (483.5 samples/s);\n",
      "Finished Epoch[13 of 80]: [Training] loss = 2.254889 * 512, metric = 83.20% * 512 0.996s (514.1 samples/s);\n",
      "Finished Epoch[14 of 80]: [Training] loss = 2.238477 * 512, metric = 83.01% * 512 0.951s (538.4 samples/s);\n",
      "Finished Epoch[15 of 80]: [Training] loss = 2.181051 * 512, metric = 74.41% * 512 0.944s (542.4 samples/s);\n",
      "Finished Epoch[16 of 80]: [Training] loss = 2.188012 * 512, metric = 81.25% * 512 0.953s (537.3 samples/s);\n",
      "Finished Epoch[17 of 80]: [Training] loss = 2.170146 * 512, metric = 75.20% * 512 0.948s (540.1 samples/s);\n",
      "Finished Epoch[18 of 80]: [Training] loss = 2.169989 * 512, metric = 76.37% * 512 0.973s (526.2 samples/s);\n",
      "Finished Epoch[19 of 80]: [Training] loss = 2.179401 * 512, metric = 78.91% * 512 0.933s (548.8 samples/s);\n",
      "Finished Epoch[20 of 80]: [Training] loss = 2.145978 * 512, metric = 76.76% * 512 0.982s (521.4 samples/s);\n",
      "Finished Epoch[21 of 80]: [Training] loss = 2.200404 * 512, metric = 82.03% * 512 0.947s (540.7 samples/s);\n",
      "Finished Epoch[22 of 80]: [Training] loss = 2.133293 * 512, metric = 77.15% * 512 1.026s (499.0 samples/s);\n",
      "Finished Epoch[23 of 80]: [Training] loss = 2.034930 * 512, metric = 66.02% * 512 1.095s (467.6 samples/s);\n",
      "Finished Epoch[24 of 80]: [Training] loss = 1.988438 * 512, metric = 70.70% * 512 0.946s (541.2 samples/s);\n",
      "Finished Epoch[25 of 80]: [Training] loss = 1.980983 * 512, metric = 72.85% * 512 0.922s (555.3 samples/s);\n",
      "Finished Epoch[26 of 80]: [Training] loss = 1.974930 * 512, metric = 67.38% * 512 0.901s (568.3 samples/s);\n",
      "Finished Epoch[27 of 80]: [Training] loss = 2.166745 * 512, metric = 79.88% * 512 1.022s (501.0 samples/s);\n",
      "Finished Epoch[28 of 80]: [Training] loss = 2.225431 * 512, metric = 76.95% * 512 0.952s (537.8 samples/s);\n",
      "Finished Epoch[29 of 80]: [Training] loss = 2.166364 * 512, metric = 75.20% * 512 1.141s (448.7 samples/s);\n",
      "Finished Epoch[30 of 80]: [Training] loss = 1.965014 * 512, metric = 61.72% * 512 0.943s (542.9 samples/s);\n",
      "Finished Epoch[31 of 80]: [Training] loss = 1.844726 * 512, metric = 66.02% * 512 0.962s (532.2 samples/s);\n",
      "Finished Epoch[32 of 80]: [Training] loss = 1.768523 * 512, metric = 60.55% * 512 1.049s (488.1 samples/s);\n",
      "Finished Epoch[33 of 80]: [Training] loss = 1.753571 * 512, metric = 66.02% * 512 0.954s (536.7 samples/s);\n",
      "Finished Epoch[34 of 80]: [Training] loss = 2.017668 * 512, metric = 71.29% * 512 1.007s (508.4 samples/s);\n",
      "Finished Epoch[35 of 80]: [Training] loss = 1.790150 * 512, metric = 55.08% * 512 0.897s (570.8 samples/s);\n",
      "Finished Epoch[36 of 80]: [Training] loss = 1.654741 * 512, metric = 58.79% * 512 0.937s (546.4 samples/s);\n",
      "Finished Epoch[37 of 80]: [Training] loss = 1.749106 * 512, metric = 62.30% * 512 0.947s (540.7 samples/s);\n",
      "Finished Epoch[38 of 80]: [Training] loss = 1.901336 * 512, metric = 64.65% * 512 1.887s (271.3 samples/s);\n",
      "Finished Epoch[39 of 80]: [Training] loss = 1.873030 * 512, metric = 63.48% * 512 1.246s (410.9 samples/s);\n",
      "Finished Epoch[40 of 80]: [Training] loss = 1.726588 * 512, metric = 57.81% * 512 1.218s (420.4 samples/s);\n",
      "Finished Epoch[41 of 80]: [Training] loss = 1.466950 * 512, metric = 49.02% * 512 1.196s (428.1 samples/s);\n",
      "Finished Epoch[42 of 80]: [Training] loss = 1.422910 * 512, metric = 49.02% * 512 1.035s (494.7 samples/s);\n",
      "Finished Epoch[43 of 80]: [Training] loss = 1.487993 * 512, metric = 52.15% * 512 0.917s (558.3 samples/s);\n",
      "Finished Epoch[44 of 80]: [Training] loss = 1.311452 * 512, metric = 42.77% * 512 1.300s (393.8 samples/s);\n",
      "Finished Epoch[45 of 80]: [Training] loss = 1.349610 * 512, metric = 45.51% * 512 1.024s (500.0 samples/s);\n",
      "Finished Epoch[46 of 80]: [Training] loss = 1.434898 * 512, metric = 50.98% * 512 0.917s (558.3 samples/s);\n",
      "Finished Epoch[47 of 80]: [Training] loss = 1.583748 * 512, metric = 56.25% * 512 1.305s (392.3 samples/s);\n",
      "Finished Epoch[48 of 80]: [Training] loss = 1.568531 * 512, metric = 55.08% * 512 1.060s (483.0 samples/s);\n",
      "Finished Epoch[49 of 80]: [Training] loss = 1.587539 * 512, metric = 52.34% * 512 0.904s (566.4 samples/s);\n",
      "Finished Epoch[50 of 80]: [Training] loss = 1.343616 * 512, metric = 44.73% * 512 1.024s (500.0 samples/s);\n",
      "Finished Epoch[51 of 80]: [Training] loss = 1.200103 * 512, metric = 42.97% * 512 1.164s (439.9 samples/s);\n",
      "Finished Epoch[52 of 80]: [Training] loss = 1.191776 * 512, metric = 41.60% * 512 1.044s (490.4 samples/s);\n",
      "Finished Epoch[53 of 80]: [Training] loss = 1.290296 * 512, metric = 45.70% * 512 1.105s (463.3 samples/s);\n",
      "Finished Epoch[54 of 80]: [Training] loss = 1.185996 * 512, metric = 39.84% * 512 1.268s (403.8 samples/s);\n",
      "Finished Epoch[55 of 80]: [Training] loss = 1.096334 * 512, metric = 36.33% * 512 1.055s (485.3 samples/s);\n",
      "Finished Epoch[56 of 80]: [Training] loss = 1.040784 * 512, metric = 33.98% * 512 0.938s (545.8 samples/s);\n",
      "Finished Epoch[57 of 80]: [Training] loss = 0.978735 * 512, metric = 34.77% * 512 1.256s (407.6 samples/s);\n",
      "Finished Epoch[58 of 80]: [Training] loss = 0.935241 * 512, metric = 30.27% * 512 1.043s (490.9 samples/s);\n",
      "Finished Epoch[59 of 80]: [Training] loss = 0.955933 * 512, metric = 33.01% * 512 0.964s (531.1 samples/s);\n",
      "Finished Epoch[60 of 80]: [Training] loss = 0.907247 * 512, metric = 32.03% * 512 1.051s (487.2 samples/s);\n",
      "Finished Epoch[61 of 80]: [Training] loss = 1.087250 * 512, metric = 36.33% * 512 1.003s (510.5 samples/s);\n",
      "Finished Epoch[62 of 80]: [Training] loss = 1.127208 * 512, metric = 38.09% * 512 1.069s (479.0 samples/s);\n",
      "Finished Epoch[63 of 80]: [Training] loss = 1.375709 * 512, metric = 50.59% * 512 0.951s (538.4 samples/s);\n",
      "Finished Epoch[64 of 80]: [Training] loss = 1.218933 * 512, metric = 40.04% * 512 1.071s (478.1 samples/s);\n",
      "Finished Epoch[65 of 80]: [Training] loss = 1.068095 * 512, metric = 33.98% * 512 1.125s (455.1 samples/s);\n",
      "Finished Epoch[66 of 80]: [Training] loss = 0.880950 * 512, metric = 26.76% * 512 1.046s (489.5 samples/s);\n",
      "Finished Epoch[67 of 80]: [Training] loss = 0.833584 * 512, metric = 28.32% * 512 0.983s (520.9 samples/s);\n",
      "Finished Epoch[68 of 80]: [Training] loss = 0.737635 * 512, metric = 24.41% * 512 1.013s (505.4 samples/s);\n",
      "Finished Epoch[69 of 80]: [Training] loss = 0.764251 * 512, metric = 25.39% * 512 0.942s (543.5 samples/s);\n",
      "Finished Epoch[70 of 80]: [Training] loss = 0.759759 * 512, metric = 25.39% * 512 0.928s (551.7 samples/s);\n",
      "Finished Epoch[71 of 80]: [Training] loss = 0.764293 * 512, metric = 24.22% * 512 1.270s (403.1 samples/s);\n",
      "Finished Epoch[72 of 80]: [Training] loss = 0.839638 * 512, metric = 27.34% * 512 1.010s (506.9 samples/s);\n",
      "Finished Epoch[73 of 80]: [Training] loss = 0.745885 * 512, metric = 24.41% * 512 1.066s (480.3 samples/s);\n",
      "Finished Epoch[74 of 80]: [Training] loss = 0.769155 * 512, metric = 24.02% * 512 1.058s (483.9 samples/s);\n",
      "Finished Epoch[75 of 80]: [Training] loss = 0.836039 * 512, metric = 29.88% * 512 1.051s (487.2 samples/s);\n",
      "Finished Epoch[76 of 80]: [Training] loss = 0.828740 * 512, metric = 24.61% * 512 1.014s (504.9 samples/s);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[77 of 80]: [Training] loss = 0.736222 * 512, metric = 23.63% * 512 1.165s (439.5 samples/s);\n",
      "Finished Epoch[78 of 80]: [Training] loss = 0.692275 * 512, metric = 22.85% * 512 1.149s (445.6 samples/s);\n",
      "Finished Epoch[79 of 80]: [Training] loss = 0.726681 * 512, metric = 23.05% * 512 1.296s (395.1 samples/s);\n",
      "Finished Epoch[80 of 80]: [Training] loss = 0.743446 * 512, metric = 23.63% * 512 1.038s (493.3 samples/s);\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNet()\n",
    "def train(epoch, batch_map):\n",
    "    cnn.train(batch_map)\n",
    "    cnn.summarize()\n",
    "cnn.log_parameters()\n",
    "cnn.evaluate(train, {'inputs': train_features, 'labels': train_labels})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
