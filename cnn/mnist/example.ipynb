{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets        import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import label_binarize\n",
    "\n",
    "PIXEL_RANGE = range(0, 255 + 1)\n",
    "IMAGE_RANGE = range(0, 9 + 1)\n",
    "\n",
    "def process(mnist):\n",
    "    # Process dataset to return features and labels for CNN\n",
    "    def features():\n",
    "        # Transform features to be float32 sets of 1x28x28 \n",
    "        # tensors with normalized pixel values.\n",
    "        return np.divide(\n",
    "            mnist.data, PIXEL_RANGE[-1]\n",
    "        ).astype(np.float32).reshape((-1, 1, 28, 28))\n",
    "    def labels():\n",
    "        # Transform labels to be float32 sets of 1x10\n",
    "        # tensors that are one-hot encoded.\n",
    "        return label_binarize(\n",
    "            mnist.target, classes=IMAGE_RANGE\n",
    "        ).astype(np.float32).reshape((-1, 10))\n",
    "    return features(), labels()\n",
    "\n",
    "\n",
    "# Download the MNIST dataset\n",
    "mnist            = fetch_mldata('MNIST original', data_home='.')\n",
    "features, labels = process(mnist)\n",
    "\n",
    "# Split train and test data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNTK 2.0 CPU'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import CNTK\n",
    "import cntk as c\n",
    "\n",
    "# Display CNTK version\n",
    "' '.join([\n",
    "    c.__name__.upper(),\n",
    "    c.__version__,\n",
    "    str(c.device.all_devices()[0])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet():\n",
    "    IMAGE_SHAPE        = (1, 28, 28)\n",
    "    IMAGE_CLASSES      = [n for n in IMAGE_RANGE]\n",
    "    IMAGE_CLASS_COUNT  = len(IMAGE_CLASSES)\n",
    "    LEARNING_RATE      = 0.001\n",
    "    EPOCH_COUNT        = 20\n",
    "    BATCH_SIZE         = 64\n",
    "    DROP_RATE          = 0.5\n",
    "    HIDDEN_LAYER_COUNT = 96\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._build_inputs()\n",
    "        self._build_layers()\n",
    "        self._build_stack()\n",
    "        self._build_trainer()\n",
    "    \n",
    "    def evaluate(self, fn, feeds):\n",
    "        c.logging.log_number_of_parameters(self.stack); print()\n",
    "        for epoch in range(self.EPOCH_COUNT):\n",
    "            batch = self.batch(epoch, feeds)\n",
    "            fn(epoch, batch)\n",
    "    \n",
    "    def train(self, data):\n",
    "        self.trainer.train_minibatch(data)\n",
    "    \n",
    "    def summarize(self):\n",
    "        self.trainer.summarize_training_progress()\n",
    "    \n",
    "    def checkpoint(self, version):\n",
    "        self.stack.save(\n",
    "            os.path.join(\n",
    "                '.', '_'.join('ConvNet', 'MNIST', '{}.dnn'.format(version))\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def batch(self, epoch, data):\n",
    "        def chunk(data):\n",
    "            slice_begin = epoch * self.BATCH_SIZE\n",
    "            slice_end   = slice_begin + self.BATCH_SIZE\n",
    "            return data[slice_begin:slice_end]\n",
    "        return {key: chunk(value) for key, value in data.items()}\n",
    "    \n",
    "    def _build_inputs(self):\n",
    "        self.inputs = c.input_variable(self.IMAGE_SHAPE,       np.float32, name='inputs')\n",
    "        self.labels = c.input_variable(self.IMAGE_CLASS_COUNT, np.float32, name='labels')\n",
    "    \n",
    "    def _build_layers(self):\n",
    "        with c.layers.default_options(activation=c.ops.relu, pad=False):\n",
    "            self.layers  = [\n",
    "                c.layers.Convolution2D((5,5), 32, pad=True),\n",
    "                c.layers.MaxPooling((3,3), (2,2)),\n",
    "                c.layers.Convolution2D((3,3), 48),\n",
    "                c.layers.MaxPooling((3,3), (2,2)),\n",
    "                c.layers.Convolution2D((3,3), 64),\n",
    "                c.layers.Dense(self.HIDDEN_LAYER_COUNT),\n",
    "                c.layers.Dropout(self.DROP_RATE),\n",
    "                c.layers.Dense(self.IMAGE_CLASS_COUNT, activation=None)\n",
    "            ]\n",
    "    \n",
    "    def _build_stack(self):\n",
    "        self.stack = self.inputs\n",
    "        for layer in self.layers: self.stack = layer(self.stack)\n",
    "        self.loss  = c.losses.cross_entropy_with_softmax(self.stack, self.labels)\n",
    "        self.error = c.metrics.classification_error(self.stack, self.labels)\n",
    "    \n",
    "    def _build_trainer(self):\n",
    "        schedule     = c.learning_rate_schedule(self.LEARNING_RATE, c.UnitType.minibatch)\n",
    "        learner      = c.learners.sgd(self.stack.parameters, schedule)\n",
    "        printer      = c.logging.ProgressPrinter(tag='Training', num_epochs=self.EPOCH_COUNT)\n",
    "        self.trainer = c.Trainer(self.stack, (self.loss, self.error), learner, printer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 98778 parameters in 10 parameter tensors.\n",
      "\n",
      "Learning rate per minibatch: 0.001\n",
      "Finished Epoch[1 of 20]: [Training] loss = 2.304523 * 64, metric = 92.19% * 64 0.468s (136.8 samples/s);\n",
      "Finished Epoch[2 of 20]: [Training] loss = 2.321438 * 64, metric = 90.62% * 64 0.121s (528.9 samples/s);\n",
      "Finished Epoch[3 of 20]: [Training] loss = 2.297114 * 64, metric = 82.81% * 64 0.110s (581.8 samples/s);\n",
      "Finished Epoch[4 of 20]: [Training] loss = 2.333561 * 64, metric = 92.19% * 64 0.158s (405.1 samples/s);\n",
      "Finished Epoch[5 of 20]: [Training] loss = 2.342719 * 64, metric = 87.50% * 64 0.151s (423.8 samples/s);\n",
      "Finished Epoch[6 of 20]: [Training] loss = 2.289195 * 64, metric = 87.50% * 64 0.105s (609.5 samples/s);\n",
      "Finished Epoch[7 of 20]: [Training] loss = 2.305540 * 64, metric = 87.50% * 64 0.146s (438.4 samples/s);\n",
      "Finished Epoch[8 of 20]: [Training] loss = 2.326389 * 64, metric = 92.19% * 64 0.103s (621.4 samples/s);\n",
      "Finished Epoch[9 of 20]: [Training] loss = 2.290867 * 64, metric = 89.06% * 64 0.101s (633.7 samples/s);\n",
      "Finished Epoch[10 of 20]: [Training] loss = 2.341750 * 64, metric = 89.06% * 64 0.214s (299.1 samples/s);\n",
      "Finished Epoch[11 of 20]: [Training] loss = 2.301433 * 64, metric = 85.94% * 64 0.146s (438.4 samples/s);\n",
      "Finished Epoch[12 of 20]: [Training] loss = 2.283262 * 64, metric = 84.38% * 64 0.105s (609.5 samples/s);\n",
      "Finished Epoch[13 of 20]: [Training] loss = 2.289724 * 64, metric = 84.38% * 64 0.146s (438.4 samples/s);\n",
      "Finished Epoch[14 of 20]: [Training] loss = 2.344799 * 64, metric = 93.75% * 64 0.130s (492.3 samples/s);\n",
      "Finished Epoch[15 of 20]: [Training] loss = 2.298910 * 64, metric = 84.38% * 64 0.148s (432.4 samples/s);\n",
      "Finished Epoch[16 of 20]: [Training] loss = 2.320122 * 64, metric = 87.50% * 64 0.105s (609.5 samples/s);\n",
      "Finished Epoch[17 of 20]: [Training] loss = 2.308783 * 64, metric = 90.62% * 64 0.125s (512.0 samples/s);\n",
      "Finished Epoch[18 of 20]: [Training] loss = 2.261453 * 64, metric = 79.69% * 64 0.117s (547.0 samples/s);\n",
      "Finished Epoch[19 of 20]: [Training] loss = 2.293402 * 64, metric = 85.94% * 64 0.188s (340.4 samples/s);\n",
      "Finished Epoch[20 of 20]: [Training] loss = 2.296556 * 64, metric = 87.50% * 64 0.332s (192.8 samples/s);\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNet()\n",
    "def train(epoch, batch_map):\n",
    "    cnn.train(batch_map)\n",
    "    cnn.summarize()\n",
    "cnn.evaluate(train, {'inputs': train_features, 'labels': train_labels})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
